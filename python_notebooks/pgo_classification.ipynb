{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "import csv\n",
    "\n",
    "import nltk\n",
    "#from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "ps = PorterStemmer()\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "os.chdir('/home/ekhongl/Codes/DL - Topic Modelling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat = pd.read_csv('data/pef.csv', header = None,skip_blank_lines=False)\n",
    "dat = list(dat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['edu', 'subject', 'com', ..., 'zero', 'aaron', 'philadelphia'], \n",
       "      dtype='<U13')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab =  np.genfromtxt('data/dtm_2000_20news.csv', dtype=str, delimiter=',', max_rows = 1)[1:]\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Data cleaning -------\n"
     ]
    }
   ],
   "source": [
    "print('------- Data cleaning -------')                \n",
    "stopwords_en = stopwords.words('english')\n",
    "dat_clean = []\n",
    "for i in range(len(dat)):\n",
    "    \n",
    "    if type(dat[i]) != float:\n",
    "        ''' tokenization and punctuation removal '''\n",
    "        # uses nltk tokenization - e.g. shouldn't = [should, n't] instead of [shouldn, 't]\n",
    "        tmp_doc = nltk.tokenize.word_tokenize(dat[i].lower())\n",
    "\n",
    "        # split words sperated by fullstops\n",
    "        tmp_doc_split = [w.split('.') for w in tmp_doc if len(w.split('.')) > 1]\n",
    "        # flatten list\n",
    "        tmp_doc_split = [i_sublist for i_list in tmp_doc_split for i_sublist in i_list]\n",
    "        # clean split words\n",
    "        tmp_doc_split = [w for w in tmp_doc_split if re.search('^[a-z]+$',w)]\n",
    "\n",
    "        # drop punctuations\n",
    "        tmp_doc_clean = [w for w in tmp_doc if re.search('^[a-z]+$',w)]\n",
    "        tmp_doc_clean.extend(tmp_doc_split)\n",
    "\n",
    "        ''' stop word removal'''\n",
    "        tmp_doc_clean_stop = [w for w in tmp_doc_clean if w not in stopwords_en]\n",
    "        #retain only words with 2 characters or more\n",
    "        tmp_doc_clean_stop = [w for w in  tmp_doc_clean_stop if len(w) >2]\n",
    "\n",
    "        ''' stemming (using the Porter's algorithm)'''\n",
    "        tmp_doc_clean_stop_stemmed = [ps.stem(w) for w in tmp_doc_clean_stop]\n",
    "        dat_clean.append(tmp_doc_clean_stop_stemmed)\n",
    "    else:\n",
    "        dat_clean.append([])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(input = 'content', lowercase = False, vocabulary = vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 2000), dtype=float64)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([] ).reshape(0,len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2000)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.fit_transform(dat_clean[i]).toarray().sum(axis=0, keepdims = True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtm = np.array([] ).reshape(0,len(vocab))\n",
    "for i in range(len(dat_clean)):\n",
    "    dtm = np.concatenate( (dtm, vec.fit_transform(dat_clean[i]).toarray().sum(axis=0, keepdims = True) ), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 2000)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams\n",
    "\n",
    "from lib.deeplearning import deepbeliefnet\n",
    "\n",
    "os.chdir('/home/ekhongl/Codes/DL - Topic Modelling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... building the model\n",
      "Building layer: 0\n",
      "   Input units: 2000\n",
      "  Output units: 500\n",
      "Building layer: 1\n",
      "   Input units: 500\n",
      "  Output units: 500\n",
      "Building layer: 2\n",
      "   Input units: 500\n",
      "  Output units: 128\n"
     ]
    }
   ],
   "source": [
    "model = deepbeliefnet( architecture = [2000, 500, 500, 128], n_outs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'params_2000/dbn_params_dropout'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_input = theano.shared(dtm.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = model.score(input = test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.54660677e-02,   8.91028285e-01,   7.98969090e-01, ...,\n",
       "          5.59706299e-04,   1.12502035e-02,   8.59574199e-01],\n",
       "       [  1.83742642e-02,   8.55456412e-01,   8.70182037e-01, ...,\n",
       "          6.76897645e-04,   8.63240100e-03,   8.46488357e-01],\n",
       "       [  1.21897878e-02,   8.65011692e-01,   9.73743439e-01, ...,\n",
       "          1.10489945e-03,   6.92097377e-03,   9.22724724e-01],\n",
       "       ..., \n",
       "       [  1.15953190e-02,   8.70056152e-01,   9.69405472e-01, ...,\n",
       "          1.02550129e-03,   6.73701521e-03,   9.29384947e-01],\n",
       "       [  1.92849617e-02,   8.88312459e-01,   8.48161757e-01, ...,\n",
       "          5.93443285e-04,   1.23798046e-02,   8.25212061e-01],\n",
       "       [  2.91127190e-02,   9.16935563e-01,   7.41483986e-01, ...,\n",
       "          6.81867590e-04,   1.28407655e-02,   7.49706447e-01]], dtype=float32)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
